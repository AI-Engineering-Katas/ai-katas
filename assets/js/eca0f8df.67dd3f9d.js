"use strict";(self.webpackChunktech_docs=self.webpackChunktech_docs||[]).push([[3664],{7814:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>d,metadata:()=>s,toc:()=>r});const s=JSON.parse('{"id":"capabilities/audio-understanding","title":"Audio Understanding","description":"Audio Understanding is the capability to analyze, process, and extract meaning from audio signals using digital signal processing and machine learning techniques.","source":"@site/docs/capabilities/audio-understanding.md","sourceDirName":"capabilities","slug":"/capabilities/audio-understanding","permalink":"/docs/capabilities/audio-understanding","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-katas/ai-building-blocks/tree/main/docs/capabilities/audio-understanding.md","tags":[],"version":"current","frontMatter":{"id":"audio-understanding","title":"Audio Understanding","sidebar_label":"Audio Understanding"}}');var t=e(4848),a=e(8453);const d={id:"audio-understanding",title:"Audio Understanding",sidebar_label:"Audio Understanding"},l="Audio Understanding",o={},r=[{value:"Related Solution Fields",id:"related-solution-fields",level:2},{value:"Overview",id:"overview",level:2},{value:"Applications",id:"applications",level:2}];function c(n){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"audio-understanding",children:"Audio Understanding"})}),"\n",(0,t.jsx)(i.p,{children:"Audio Understanding is the capability to analyze, process, and extract meaning from audio signals using digital signal processing and machine learning techniques."}),"\n",(0,t.jsx)(i.h2,{id:"related-solution-fields",children:"Related Solution Fields"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"../solutions/speech-recognition",children:"Speech Recognition"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"../solutions/audio-classification",children:"Audio Classification"})}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(i.p,{children:"Audio Understanding enables systems to:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Convert speech to text"}),"\n",(0,t.jsx)(i.li,{children:"Identify speakers and sounds"}),"\n",(0,t.jsx)(i.li,{children:"Analyze acoustic properties"}),"\n",(0,t.jsx)(i.li,{children:"Detect emotions in speech"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"applications",children:"Applications"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Voice assistants"}),"\n",(0,t.jsx)(i.li,{children:"Audio content moderation"}),"\n",(0,t.jsx)(i.li,{children:"Music analysis"}),"\n",(0,t.jsx)(i.li,{children:"Security and surveillance"}),"\n"]})]})}function u(n={}){const{wrapper:i}={...(0,a.R)(),...n.components};return i?(0,t.jsx)(i,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>d,x:()=>l});var s=e(6540);const t={},a=s.createContext(t);function d(n){const i=s.useContext(a);return s.useMemo((function(){return"function"==typeof n?n(i):{...i,...n}}),[i,n])}function l(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:d(n.components),s.createElement(a.Provider,{value:i},n.children)}}}]);