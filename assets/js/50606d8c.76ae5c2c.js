"use strict";(self.webpackChunktech_docs=self.webpackChunktech_docs||[]).push([[3479],{1261:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"capabilities/reinforcement-learning","title":"Reinforcement Learning","description":"Reinforcement Learning is the capability to train agents to make sequential decisions through interaction with an environment.","source":"@site/docs/capabilities/reinforcement-learning.md","sourceDirName":"capabilities","slug":"/capabilities/reinforcement-learning","permalink":"/ai-katas/capabilities/reinforcement-learning","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"reinforcement-learning","title":"Reinforcement Learning","sidebar_label":"Reinforcement Learning"},"sidebar":"defaultSidebar","previous":{"title":"Recommendation","permalink":"/ai-katas/capabilities/recommendation"},"next":{"title":"Similarity Search","permalink":"/ai-katas/capabilities/similarity-search"}}');var r=i(4848),a=i(8453);const o={id:"reinforcement-learning",title:"Reinforcement Learning",sidebar_label:"Reinforcement Learning"},s="Reinforcement Learning",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Applications",id:"applications",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"reinforcement-learning",children:"Reinforcement Learning"})}),"\n",(0,r.jsx)(n.p,{children:"Reinforcement Learning is the capability to train agents to make sequential decisions through interaction with an environment."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Reinforcement Learning enables systems to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Learn optimal policies"}),"\n",(0,r.jsx)(n.li,{children:"Adapt to changing environments"}),"\n",(0,r.jsx)(n.li,{children:"Balance exploration and exploitation"}),"\n",(0,r.jsx)(n.li,{children:"Optimize long-term rewards"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Game playing"}),"\n",(0,r.jsx)(n.li,{children:"Robot control"}),"\n",(0,r.jsx)(n.li,{children:"Resource management"}),"\n",(0,r.jsx)(n.li,{children:"Process optimization"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var t=i(6540);const r={},a=t.createContext(r);function o(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);